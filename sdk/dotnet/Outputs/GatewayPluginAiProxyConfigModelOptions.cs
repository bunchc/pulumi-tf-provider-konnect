// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Konnect.Outputs
{

    [OutputType]
    public sealed class GatewayPluginAiProxyConfigModelOptions
    {
        /// <summary>
        /// Defines the schema/API version, if using Anthropic provider.
        /// </summary>
        public readonly string? AnthropicVersion;
        /// <summary>
        /// 'api-version' for Azure OpenAI instances.
        /// </summary>
        public readonly string? AzureApiVersion;
        /// <summary>
        /// Deployment ID for Azure OpenAI instances.
        /// </summary>
        public readonly string? AzureDeploymentId;
        /// <summary>
        /// Instance name for Azure OpenAI hosted models.
        /// </summary>
        public readonly string? AzureInstance;
        /// <summary>
        /// If using llama2 provider, select the upstream message format. must be one of ["raw", "openai", "ollama"]
        /// </summary>
        public readonly string? Llama2Format;
        /// <summary>
        /// Defines the max_tokens, if using chat or completion models.
        /// </summary>
        public readonly int? MaxTokens;
        /// <summary>
        /// If using mistral provider, select the upstream message format. must be one of ["openai", "ollama"]
        /// </summary>
        public readonly string? MistralFormat;
        /// <summary>
        /// Defines the matching temperature, if using chat or completion models.
        /// </summary>
        public readonly double? Temperature;
        /// <summary>
        /// Defines the top-k most likely tokens, if supported.
        /// </summary>
        public readonly int? TopK;
        /// <summary>
        /// Defines the top-p probability mass, if supported.
        /// </summary>
        public readonly double? TopP;
        /// <summary>
        /// Manually specify or override the AI operation path, used when e.g. using the 'preserve' route_type.
        /// </summary>
        public readonly string? UpstreamPath;
        /// <summary>
        /// Manually specify or override the full URL to the AI operation endpoints, when calling (self-)hosted models, or for running via a private endpoint.
        /// </summary>
        public readonly string? UpstreamUrl;

        [OutputConstructor]
        private GatewayPluginAiProxyConfigModelOptions(
            string? anthropicVersion,

            string? azureApiVersion,

            string? azureDeploymentId,

            string? azureInstance,

            string? llama2Format,

            int? maxTokens,

            string? mistralFormat,

            double? temperature,

            int? topK,

            double? topP,

            string? upstreamPath,

            string? upstreamUrl)
        {
            AnthropicVersion = anthropicVersion;
            AzureApiVersion = azureApiVersion;
            AzureDeploymentId = azureDeploymentId;
            AzureInstance = azureInstance;
            Llama2Format = llama2Format;
            MaxTokens = maxTokens;
            MistralFormat = mistralFormat;
            Temperature = temperature;
            TopK = topK;
            TopP = topP;
            UpstreamPath = upstreamPath;
            UpstreamUrl = upstreamUrl;
        }
    }
}
