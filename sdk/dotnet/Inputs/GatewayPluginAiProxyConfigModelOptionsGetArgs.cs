// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Konnect.Inputs
{

    public sealed class GatewayPluginAiProxyConfigModelOptionsGetArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// Defines the schema/API version, if using Anthropic provider.
        /// </summary>
        [Input("anthropicVersion")]
        public Input<string>? AnthropicVersion { get; set; }

        /// <summary>
        /// 'api-version' for Azure OpenAI instances.
        /// </summary>
        [Input("azureApiVersion")]
        public Input<string>? AzureApiVersion { get; set; }

        /// <summary>
        /// Deployment ID for Azure OpenAI instances.
        /// </summary>
        [Input("azureDeploymentId")]
        public Input<string>? AzureDeploymentId { get; set; }

        /// <summary>
        /// Instance name for Azure OpenAI hosted models.
        /// </summary>
        [Input("azureInstance")]
        public Input<string>? AzureInstance { get; set; }

        /// <summary>
        /// If using llama2 provider, select the upstream message format. must be one of ["raw", "openai", "ollama"]
        /// </summary>
        [Input("llama2Format")]
        public Input<string>? Llama2Format { get; set; }

        /// <summary>
        /// Defines the max_tokens, if using chat or completion models.
        /// </summary>
        [Input("maxTokens")]
        public Input<int>? MaxTokens { get; set; }

        /// <summary>
        /// If using mistral provider, select the upstream message format. must be one of ["openai", "ollama"]
        /// </summary>
        [Input("mistralFormat")]
        public Input<string>? MistralFormat { get; set; }

        /// <summary>
        /// Defines the matching temperature, if using chat or completion models.
        /// </summary>
        [Input("temperature")]
        public Input<double>? Temperature { get; set; }

        /// <summary>
        /// Defines the top-k most likely tokens, if supported.
        /// </summary>
        [Input("topK")]
        public Input<int>? TopK { get; set; }

        /// <summary>
        /// Defines the top-p probability mass, if supported.
        /// </summary>
        [Input("topP")]
        public Input<double>? TopP { get; set; }

        /// <summary>
        /// Manually specify or override the AI operation path, used when e.g. using the 'preserve' route_type.
        /// </summary>
        [Input("upstreamPath")]
        public Input<string>? UpstreamPath { get; set; }

        /// <summary>
        /// Manually specify or override the full URL to the AI operation endpoints, when calling (self-)hosted models, or for running via a private endpoint.
        /// </summary>
        [Input("upstreamUrl")]
        public Input<string>? UpstreamUrl { get; set; }

        public GatewayPluginAiProxyConfigModelOptionsGetArgs()
        {
        }
        public static new GatewayPluginAiProxyConfigModelOptionsGetArgs Empty => new GatewayPluginAiProxyConfigModelOptionsGetArgs();
    }
}
